{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ac48658-c6a6-413e-8756-422153d887d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "575b9f8d-6b89-4b86-ab0e-affffc989e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the most frequent types of day-to-day conversion is text communication. In our everyday routine, we chat, message, tweet, share status, email, create blogs, and offer opinions and criticism. All of these actions lead to a substantial amount of unstructured text being produced. It is critical to examine huge amounts of data in this sector of the online world and social media to determine people's opinions. Text mining is also referred to as text analytics. Text mining is a process of exploring sizable textual data and finding patterns. Text Mining processes the text itself, while NLP processes with the underlying metadata. Finding frequency counts of words, length of the sentence, presence/absence of specific words is known as text mining. Natural language processing is one of the components of text mining. NLP helps identify sentiment, finding entities in the sentence, and category of blog/article. Text mining is preprocessed data for text analytics. In Text Analytics, statistical and machine learning algorithms are used to classify information.\n",
      "\n",
      "\n",
      "['One of the most frequent types of day-to-day conversion is text communication.', 'In our everyday routine, we chat, message, tweet, share status, email, create blogs, and offer opinions and criticism.', 'All of these actions lead to a substantial amount of unstructured text being produced.', \"It is critical to examine huge amounts of data in this sector of the online world and social media to determine people's opinions.\", 'Text mining is also referred to as text analytics.', 'Text mining is a process of exploring sizable textual data and finding patterns.', 'Text Mining processes the text itself, while NLP processes with the underlying metadata.', 'Finding frequency counts of words, length of the sentence, presence/absence of specific words is known as text mining.', 'Natural language processing is one of the components of text mining.', 'NLP helps identify sentiment, finding entities in the sentence, and category of blog/article.', 'Text mining is preprocessed data for text analytics.', 'In Text Analytics, statistical and machine learning algorithms are used to classify information.']\n",
      "\n",
      "\n",
      "['One', 'of', 'the', 'most', 'frequent', 'types', 'of', 'day-to-day', 'conversion', 'is', 'text', 'communication', '.', 'In', 'our', 'everyday', 'routine', ',', 'we', 'chat', ',', 'message', ',', 'tweet', ',', 'share', 'status', ',', 'email', ',', 'create', 'blogs', ',', 'and', 'offer', 'opinions', 'and', 'criticism', '.', 'All', 'of', 'these', 'actions', 'lead', 'to', 'a', 'substantial', 'amount', 'of', 'unstructured', 'text', 'being', 'produced', '.', 'It', 'is', 'critical', 'to', 'examine', 'huge', 'amounts', 'of', 'data', 'in', 'this', 'sector', 'of', 'the', 'online', 'world', 'and', 'social', 'media', 'to', 'determine', 'people', \"'s\", 'opinions', '.', 'Text', 'mining', 'is', 'also', 'referred', 'to', 'as', 'text', 'analytics', '.', 'Text', 'mining', 'is', 'a', 'process', 'of', 'exploring', 'sizable', 'textual', 'data', 'and', 'finding', 'patterns', '.', 'Text', 'Mining', 'processes', 'the', 'text', 'itself', ',', 'while', 'NLP', 'processes', 'with', 'the', 'underlying', 'metadata', '.', 'Finding', 'frequency', 'counts', 'of', 'words', ',', 'length', 'of', 'the', 'sentence', ',', 'presence/absence', 'of', 'specific', 'words', 'is', 'known', 'as', 'text', 'mining', '.', 'Natural', 'language', 'processing', 'is', 'one', 'of', 'the', 'components', 'of', 'text', 'mining', '.', 'NLP', 'helps', 'identify', 'sentiment', ',', 'finding', 'entities', 'in', 'the', 'sentence', ',', 'and', 'category', 'of', 'blog/article', '.', 'Text', 'mining', 'is', 'preprocessed', 'data', 'for', 'text', 'analytics', '.', 'In', 'Text', 'Analytics', ',', 'statistical', 'and', 'machine', 'learning', 'algorithms', 'are', 'used', 'to', 'classify', 'information', '.']\n"
     ]
    }
   ],
   "source": [
    "file = open(\"Text.txt\",'r')\n",
    "text = file.read()\n",
    "\n",
    "print(text)\n",
    "print('\\n')\n",
    "\n",
    "tokens_sent = nltk.sent_tokenize(text)\n",
    "print(tokens_sent)\n",
    "print('\\n')\n",
    "\n",
    "tokens_words = nltk.word_tokenize(text)\n",
    "print(tokens_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d24139f6-d972-4627-8c56-198a9b94b94d",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Asus/nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tagged \u001b[38;5;241m=\u001b[39m pos_tag(tokens_words)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tagged)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m     tagger\u001b[38;5;241m.\u001b[39mload(ap_russian_model_loc)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m PerceptronTagger()\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[0;32m    166\u001b[0m     AP_MODEL_LOC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m--> 167\u001b[0m         find(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtaggers/averaged_perceptron_tagger/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m PICKLE)\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Asus/nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "tagged = pos_tag(tokens_words)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bac50110-bf34-4f98-9d63-351d2166f1c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m filtered_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens_words \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string\u001b[38;5;241m.\u001b[39mpunctuation]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiltered Tokens (Stopwords Removed):\u001b[39m\u001b[38;5;124m\"\u001b[39m, filtered_tokens)\n",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m filtered_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens_words \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string\u001b[38;5;241m.\u001b[39mpunctuation]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiltered Tokens (Stopwords Removed):\u001b[39m\u001b[38;5;124m\"\u001b[39m, filtered_tokens)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'string' is not defined"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens_words if word.lower() not in stop_words and word not in string.punctuation]\n",
    "print(\"Filtered Tokens (Stopwords Removed):\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a9bae83-aa50-46c4-84a0-648c8afd9cc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m PorterStemmer()\n\u001b[1;32m----> 2\u001b[0m stemmed_tokens \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m filtered_tokens]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStemmed Tokens:\u001b[39m\u001b[38;5;124m\"\u001b[39m, stemmed_tokens)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filtered_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac5a73ca-13e1-442a-88e3-c5c9f770e2f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m----> 2\u001b[0m lemmatized_tokens \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m filtered_tokens]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLemmatized Tokens:\u001b[39m\u001b[38;5;124m\"\u001b[39m, lemmatized_tokens)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filtered_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "106546b0-8f98-467a-ab55-ae013482fe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Representation:\n",
      "      absence   actions  algorithms       all      also    amount   amounts  \\\n",
      "0   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "2   0.000000  0.306501    0.000000  0.306501  0.000000  0.306501  0.000000   \n",
      "3   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.233516   \n",
      "4   0.000000  0.000000    0.000000  0.000000  0.444878  0.000000  0.000000   \n",
      "5   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "6   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "7   0.245772  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "8   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "9   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "10  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "11  0.000000  0.000000    0.316308  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "    analytics       and       are  ...     tweet     types  underlying  \\\n",
      "0    0.000000  0.000000  0.000000  ...  0.000000  0.289064    0.000000   \n",
      "1    0.000000  0.301948  0.000000  ...  0.244513  0.000000    0.000000   \n",
      "2    0.000000  0.000000  0.000000  ...  0.000000  0.000000    0.000000   \n",
      "3    0.000000  0.144184  0.000000  ...  0.000000  0.000000    0.000000   \n",
      "4    0.337501  0.000000  0.000000  ...  0.000000  0.000000    0.000000   \n",
      "5    0.000000  0.224289  0.000000  ...  0.000000  0.000000    0.000000   \n",
      "6    0.000000  0.000000  0.000000  ...  0.000000  0.000000    0.287484   \n",
      "7    0.000000  0.000000  0.000000  ...  0.000000  0.000000    0.000000   \n",
      "8    0.000000  0.000000  0.000000  ...  0.000000  0.000000    0.000000   \n",
      "9    0.000000  0.190722  0.000000  ...  0.000000  0.000000    0.000000   \n",
      "10   0.357253  0.000000  0.000000  ...  0.000000  0.000000    0.000000   \n",
      "11   0.239963  0.195304  0.316308  ...  0.000000  0.000000    0.000000   \n",
      "\n",
      "    unstructured      used        we     while      with     words     world  \n",
      "0       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "1       0.000000  0.000000  0.244513  0.000000  0.000000  0.000000  0.000000  \n",
      "2       0.306501  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "3       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.233516  \n",
      "4       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "5       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "6       0.000000  0.000000  0.000000  0.287484  0.287484  0.000000  0.000000  \n",
      "7       0.000000  0.000000  0.000000  0.000000  0.000000  0.491543  0.000000  \n",
      "8       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "9       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "10      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "11      0.000000  0.316308  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "\n",
      "[12 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(tokens_sent)\n",
    "\n",
    "# Convert TF-IDF matrix to DataFrame for better visualization\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"\\nTF-IDF Representation:\\n\", df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bdfe9e-b2b8-4735-8d3a-9d099eb99f30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
